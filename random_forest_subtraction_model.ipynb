{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\einwo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "num_features = 300\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the question and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(questions, model, num_features):\n",
    "    # Given a set of questions (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    questionFeatureVecs = np.zeros((len(questions),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for question in questions:\n",
    "        #\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\" % (counter, len(questions)))\n",
    "        # \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        questionFeatureVecs[counter] = makeFeatureVec(question, model, num_features)\n",
    "        #\n",
    "        # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return questionFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def question_to_wordlist( question, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "#     review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-alphanumeric characters\n",
    "    question_text = re.sub(\"[^a-zA-Z0-9]\",\" \", question)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = question_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "first_question = []\n",
    "for question in train[\"question1\"]:\n",
    "    first_question.append( question_to_wordlist( str(question) ))\n",
    "\n",
    "second_question = []\n",
    "for question in train[\"question2\"]:\n",
    "    second_question.append( question_to_wordlist( str(question) ))\n",
    "\n",
    "trainData1 = getAvgFeatureVecs( first_question, model, num_features )\n",
    "\n",
    "trainData2 = getAvgFeatureVecs( second_question, model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "test_question_1 = []\n",
    "for question in test[\"question1\"]:\n",
    "    test_question_1.append( question_to_wordlist( question ))\n",
    "test_question_2 = []\n",
    "for question in test[\"question2\"]:\n",
    "    test_question_2.append( question_to_wordlist( question ))\n",
    "\n",
    "testData1 = getAvgFeatureVecs( test_question_1, model, num_features )\n",
    "\n",
    "testData2 = getAvgFeatureVecs( test_question_2, model, num_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec model converts a word to 300 feature vector. To find the vector a sentence, add all of its word vectors together. Right now the way I find relations between the two vectors is subtracting one from the other, but I'm sure there are better and more novel ways to do this. Also, we can try XGBoost instead of RandomForest. This is what I want you to experiment with. Let me know if you have anything else you'd think would be good to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit( trainData2 - trainData1, train[\"is_duplicate\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testData2 - testData1 )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"test_id\":test[\"test_id\"], \"is_duplicate\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
